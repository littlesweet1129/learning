{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fundamentals of Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 branches of machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* supervised learning\n",
    "    - binary and multiclass classification\n",
    "    - scalar regression\n",
    "    - sequence generation: given a picture, predict a caption describing it.\n",
    "    - syntax tree prediction: given a sentence, predict its decomposition into a syntax tree.\n",
    "    - object detection: given a picture, draw a bouding box around certain objects inside the picture.\n",
    "    - image segmentation: given a picture, draw a pixel-level mask on a specific object.\n",
    "    \n",
    "* unsupervised learning\n",
    "    - finding interesting transformations of the input dat without the help of any targets, for data visualization, data compression or data denoising or better understand the correlations present in the data.\n",
    "    - dimensionality reduction\n",
    "    - clustering\n",
    "    \n",
    "* self-supervised learning\n",
    "    - supervised learning without human-annotated labels.\n",
    "    - autoencoders: a well-known instance where the generated targets are the input.\n",
    "    - temporally supervised learning: predict next frame, given past frames, or next work in a text, given previous words.\n",
    "    \n",
    "* reinforcement learning\n",
    "    - agent receives information about its environment and learns to choose action that will maximize some reward.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating ML models\n",
    "\n",
    "- split the data into: training, validation and test.\n",
    "- hyper-parameter tuning on the validation\n",
    "- information leaks: hyperparameter tune leaks validation data into the model.\n",
    "- simple hold out validation\n",
    "    - train, validation, test\n",
    "    - not good with small data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "num_validation_samples = 10000\n",
    "np.random.shuffle(data)\n",
    "validation_data = data[:num_validation_samples]\n",
    "data = data[num_validation_samples:]\n",
    "training_data = data[:]\n",
    "model = get_model()\n",
    "model.train(training_data)\n",
    "validation_score = model.evaluate(validation_data)\n",
    "# model tuning\n",
    "model = get_model()\n",
    "model.train(np.concatenate([training_data, validation_data]))\n",
    "test_score = model.evaluate(test_data)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- k-fold validation\n",
    "```python\n",
    "k=4\n",
    "num_validation_samples = len(data)//4\n",
    "np.random.shuffle(data)\n",
    "validation_scores = []\n",
    "for fold in range(k):\n",
    "    validation_data = data[num_validation_sample*fold:num_validation_sample*(fold+1)]\n",
    "    training_data = data[:num_validation_sample*fold]+data[num_validation_sample*(fold+1):]\n",
    "    model = get_model()\n",
    "    model.train(training_data)\n",
    "    validation_score = model.evalluate(validation_data)\n",
    "    validation_scores.append(validation_score)\n",
    "validation_score = np.average(validation_scores)\n",
    "model = get_model()\n",
    "model.train(data)\n",
    "test_score = model.evaluate(test_data)\n",
    "```\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- iterated k-fold validation with shuffling\n",
    "    - for situations in which you have relatively little data available and you nee to evaluate the model as precisely as possible\n",
    "    - applying k-fold validation multiple times, shuffling the data every time before splitting it k ways. \n",
    "    - final score is the average of the scores obtained at each run of k-fold validation. \n",
    "    \n",
    "Things to keep in mind:\n",
    "- data representation: random shuffle\n",
    "- arrow of time: do not random shuffle if tring to predict the future given the past\n",
    "- redundancy in data: ex. data points appear twice,, make sure training and validation are disjoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessng, feature engineering and feature learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing:\n",
    "- vectorization: input and targets need to be tensors of flaoting-point data\n",
    "- normalization: \n",
    "    - take small values: typically 0-1\n",
    "    - homogenous: all features take values in roughly the same range\n",
    "    - normalize each feature independently to have a mean 0 and std 1\n",
    "\n",
    "```python\n",
    "x -= x.mean(axis=0)\n",
    "x /= x.std(acis=0)\n",
    "```\n",
    "\n",
    "- handling missing values\n",
    "    - with neural network, its safe to input missing values as 0, with the condition that 0 is not already a meaningful value. The network will learn from exposure that 0 means missing.\n",
    "    - if test has missing, but training does not, model will not leaned to ignore missing values. One should artificially generate training samples with missing entries.\n",
    "- feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting and Underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "regularization:\n",
    "- reducing the network size \n",
    "    - number of layers\n",
    "    - number of units per layer\n",
    "    - learnable parameters -> capacity\n",
    "    - start with few layers and parameters and increase the size until seeing diminishing return wrt validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000, )))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smaller model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(4, activation='relu', input_shape=(10000, )))\n",
    "model.add(layers.Dense(4, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# high capacity model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(512, activation='relu', input_shape=(10000, )))\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "# bigger model overfit early, noisier validation loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- adding weight regularization\n",
    "    - Occam;s razor: given two explanations for something, the one most likely to be correct is the simpler one- the one with fewer assumptions.\n",
    "    - simpler models are less likely to overfit than complex ones\n",
    "    - simple model is a model where the distribution of parameters values has less entropy\n",
    "    - put constraints on the complexity of a network by forcing its weights to take only small values.\n",
    "    - weight regularization\n",
    "    - add to the loss function a cost associated with having large weights\n",
    "    - L1\n",
    "    - L2\n",
    "    - pass weight relularizer instances to layers as keword arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001), activation='relu', input_shape=(10000, )))\n",
    "model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001), activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`l2(0.001)` means every coefficient will add `0.001* weight_coeffcient_value` to the total loss of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.regularizers.L1L2 at 0x7f1114b4e4e0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "regularizers.l1(0.001) # L1 regularization\n",
    "regularizers.l1_l2(l1=0.001, l2=0.001) # simultaneous L1 and L2 relularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- adding dropout\n",
    "    - dropout, applied to a layer, consists of randomly dropping out a number of output features of the layer during training.\n",
    "    - dropout rate is the fraction of features that zre zeroed out, usually in (0.2, 0.5)\n",
    "    - at test no units are dropped out, but the layer's output values are scaled down by a factor equal to the dropout rate to balance (or scale up in training)\n",
    "    - introduce dropout via `Dropout` layer\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer_output = * np.random.randint(0, high=2, size = layer_output.shape)\n",
    "# layer_output /= 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16,  activation='relu', input_shape=(10000, )))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(16,  activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To recap, most common ways to prevent overfitting in neural network:\n",
    "- get more training data\n",
    "- reducing the capacity of the network\n",
    "- add weight regularization\n",
    "- add dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Universal workflow of machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- defining the problem and assembling the datasets\n",
    "    - what is input data\n",
    "    - what is target\n",
    "    - what type of model\n",
    "- choosing a measure of success\n",
    "    - metric of success will guide the choice of loss function\n",
    "    - for balanced-classification problem: accuracy, roc\n",
    "    - for ranking problems or multilabel classification: mean average precision\n",
    "- deciding on an evaluation protocol\n",
    "    - maintain a hold-out validation set (for most of the cases)\n",
    "    - doing k-fold cross-validation\n",
    "    - doing iterated k-fold validation\n",
    "- preparing the data\n",
    "    - once you know what you are training on, what you are optimizing for and how to evaluate your approach\n",
    "    - format data in a way that can be fed into a machine-learning model\n",
    "        - formatted as tensors\n",
    "        - values be scaled to small values in $[-1, 1]$ or $[0, 1]$ range\n",
    "        - normalize if different feature takes different ranges\n",
    "        - feature engineering especially for small-data problems\n",
    "- developing a first model that does better than a baseline: a model wtih statistical power\n",
    "    - hypotheses:\n",
    "        - output can be predicted given inputs\n",
    "        - available data is sufficiently informative to learn the relationship between input and output\n",
    "    - key choices:\n",
    "        - last-layer activation: to establish useful constraints on the network's output\n",
    "        - loss function: mathc the type of problem trying to solve\n",
    "        - optimization configuration: what optimizer at what learning rate, mostly safe to use `rmsprop` and its default learning rate\n",
    "        \n",
    "Note: loss funciton \n",
    "    - need to be computable given only a mini-batch of data(ideally a single data point)\n",
    "    - must be differentiable \n",
    "<img src = 'loss_activate.png'>\n",
    "\n",
    "- scaling up: developing a model that overfits\n",
    "    - to figure out how big a model you will need, first develoo one that overfits\n",
    "        - add layers\n",
    "        - make the layers bigger\n",
    "        - train for more epochs\n",
    "    - monitor the train and validation loss and evaluation metrics\n",
    "- regularizing your model and tuning your parameters\n",
    "    - add dropout\n",
    "    - try different architectures: add or remove layers\n",
    "    - add l1 and/l2 regularization\n",
    "    - try different hyperparameters\n",
    "        - number of units per layer\n",
    "        - learning rate\n",
    "    - iterate on feature engineering\n",
    "        - add new features\n",
    "        - remove features that dont seem to be informative\n",
    "    - feedback from validatio process to tune model is leaking information from validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
